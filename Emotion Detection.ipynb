{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6d485c8",
   "metadata": {},
   "source": [
    "## Face Emotion Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f8a24b",
   "metadata": {},
   "source": [
    "The data consists of 48x48 pixel grayscale images of faces. The faces have been automatically registered so that the face is more or less centered and occupies about the same amount of space in each image. The task is to categorize each face based on the emotion shown in the facial expression in to one of seven categories (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8facb5",
   "metadata": {},
   "source": [
    "For our model we will be considering only the distinct Emotions which are (0=Angry, 3=Happy, 4=Sad, 5=Surprise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "355818bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "437851dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./fer2013.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5060b8",
   "metadata": {},
   "source": [
    "### Data exploration and preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e934c432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels     Usage\n",
       "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
       "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
       "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
       "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
       "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07e66cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 35887 entries, 0 to 35886\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   emotion  35887 non-null  int64 \n",
      " 1   pixels   35887 non-null  object\n",
      " 2   Usage    35887 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 841.2+ KB\n",
      "35887\n"
     ]
    }
   ],
   "source": [
    "df.info()\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09206963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Training       28709\n",
       "PublicTest      3589\n",
       "PrivateTest     3589\n",
       "Name: Usage, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Usage'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60391605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting dataset into training and testing \n",
    "train_data = df[df[\"Usage\"] == \"Training\"]\n",
    "public_test = df[df[\"Usage\"] == \"PublicTest\"]\n",
    "public_test = public_test.reset_index()\n",
    "private_test = df[df[\"Usage\"] == \"PrivateTest\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6139eca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    8989\n",
       "6    6198\n",
       "4    6077\n",
       "2    5121\n",
       "0    4953\n",
       "5    4002\n",
       "1     547\n",
       "Name: emotion, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0097ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_dict = { 0:\"Angry\", 3:\"Happy\", 4:\"Sad\", 5:\"Surprise\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f01027d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting requried emotions\n",
    "train_data = train_data[(train_data[\"emotion\"]==0) | (train_data[\"emotion\"]==3) | (train_data[\"emotion\"]==4) | (train_data[\"emotion\"]==5) ]\n",
    "train_data = train_data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c39c5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_pic_with_emotion(raw_pixels,emotion,emotion_dict):\n",
    "    img = raw_pixels\n",
    "    val = img.split(\" \")\n",
    "    pixels = np.array(val, 'float32')\n",
    "    pixels /= 255\n",
    "    \n",
    "    x_reshaped = pixels.reshape(48,48)\n",
    "    plt.imshow(x_reshaped, cmap= \"gray\",\n",
    "              interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    print(emotion_dict.get(emotion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a12858a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAWb0lEQVR4nO2d0YtVVRvGV5ZpWdlMluNkIChaktGV10HRXfTnBV111T9Q0E1BIARFA1KUKQ6ZllaalmVNTel3NfDtZ/3O2c/ZY3wvfs/vbp9ZZ++91j4ve55nvetd9925c6eFEOqx4399AyEEJsEZQlESnCEUJcEZQlESnCEU5YF5f9y1a9eolfvPP/9Ynyn333//4PjRRx8dbUM8+OCDo+c5evTo4PiBB/puP/TQQ91nu3btGm3z8MMPj96j3pPeM51nSt9ba+3xxx8fvT6d+9dffx0c//bbb12bv//+e3Cs40P3RNf666+/us/GWFtb6z576623us+0//R70H7QM9R73L17d9dmZWVlcPznn392bfTc9Mzefvvt+7oPW96cIZQlwRlCURKcIRQlwRlCUeYaQmTsqMB3zB9CzRUSyr///vvg2DGNqM0Us4Vwvkem0RRzh0wKNRzIkCH0Gem4UpsnnnjCOreifaW+67XUjKI2Tz31VNfmwIED3Wc//vjj4PjJJ5/s2vz8889zr9Vaf9+bm5tdGxrHsWtRP2aRN2cIRUlwhlCUBGcIRZmrOQn9/9zRU9TG0UuaLEB6Uv/vdzQfXVsnpul7d4upOl0nxqkfNEaqZ0kr6bnIA1BtSMkcY9eme6SkBNXXpIFXV1e7z1TjOfdIz0OTDkhzOv7LrVu3Bsd//PHH6P1skTdnCEVJcIZQlARnCEVJcIZQlLlq2TFEpqxAaa03BWgimswERSfrnRUGZKTQZ07/1big8VBTwunXI4880n2mphWtgnAm9MkQ0jGic+t4UD/0WmSA6LVonB0T75lnnuk+W19f7z5T9uzZMzhW06a1fnULGULaV1oRpGOthtU88uYMoSgJzhCKkuAMoSgLJyE46P/ipAMd3eVM8qp2da7lJhc4+knPTfeoOtDRWIRzHifxn3AqSmjffvrpp66NJoGQVlTNSQkgTrILJZFTEoqi2tDxTZwEGNKcOvY0ZrPImzOEoiQ4QyhKgjOEoiQ4QyjKtishEI65oOYGXUvFNLXRaznmAk2wk3ExZQXO1FKhahQ43yGDiia59b5pFYhj0F2+fHn0+s4KGOd+FDK1nJUqNB6aFEJlLxXn2VNf9b5v3rw5ep4t8uYMoSgJzhCKkuAMoSjbroRA2kgnY+l/etUCU6vW6bWcCXfC0ddOJbmp11KtRhpQtRLdD+kwHSNqo1XrLl261LVRXTx1qwUnuUTb0EIASnzXvpEOVF+CqiVsbGwMjuk37JxH+/HYY491bWaRN2cIRUlwhlCUBGcIRUlwhlCUhQ0hxyRRUX779u2ujZOooGJ+3759k+5HcaoVUDtnot4pfUimFfV/DHc7BjVynGoJdI9O+cyx87bmPTMnUYHuUc0vMrb0WVPiiu5POjVRQcdokb1J8+YMoSgJzhCKkuAMoSgLa079n5lWf+/YMYz569evd20cbajagK51t5iqVads2UAT6npu0pPafzcpQjWns2UDoW0omcFJAnGqATqa003CUEhjKo4u1Ta0oELH2tk2cIu8OUMoSoIzhKIkOEMoSoIzhKIsXAlBV4+QuaETuE6VA5oYV3E/taSkM1k/1RBSk4RMKzUTnHGl8xw4cGBwrCtJZqGmhPPMnFUgZCxpXykpw9nWwYHMFR03evZqkNFYa9IB9UN/n5TIon2jcp6zyJszhKIkOEMoSoIzhKIsnITgJGg7yd/ORPD+/fsHx1O2cKDvURvSJs5WCzoedG7VRnQtp69ONUD6TLUpVZ1wtLP2dap21t8HTd5rpQxqQ4kJeo+UlKDJ8JTUrvqRqvipBneqeSySSJM3ZwhFSXCGUJQEZwhFSXCGUJS5hpCz1yGJchXvNDmr31teXu7aTEkwcCaCHfOnNW87CL3HH374YfQ8hw8f7troZ3Qtx1gi48SpPKDncow/WoGivxlqowknlICiOCZja33fDh061LX57LPPBseagNFab/ZQwoeams4qIbd6RWt5c4ZQlgRnCEVJcIZQlLma0ym37+hC53/xzc3N0TakC0ljjbVxNadOPNMEst7j+vp610YTDNbW1ro2OjF+4sSJro0z1qSNnEqH2g/SgarnqbKdXp80pyZ/UzK4buWnyeqzUF1O59bnSJU6dDy0ukdrvVZ19OQiVRbz5gyhKAnOEIqS4AyhKAnOEIoy1xCiCX3F2Z/SqSCwc+fOro0aIDRZrAYACXdddUDJFSTU9fpkSly8eHFwTObClStX5t5Pa/2+jZTc8fzzzw+OaWKeVk8cPXq0+0zRZ0TGko7H6dOnuzZqdlG1Ap3gP378eNdGkwfIoHKSIGgLj5WVlcHx1atXuzY6/vTM9J7od+VsczGLvDlDKEqCM4SiJDhDKEqCM4SizHV8nOybKXuFELSHpxowZHZotgdl8ai4J4OKDAf97Nq1a10bp6SkjtGePXu6NnpPZLbo86B7dspgOEYSjfXly5cHx2QaqbFGxta5c+cGx19//XXX5siRI4NjzbJqjbO6nN+Dlhj98ssvuzZqZE39net5nBU4W+TNGUJREpwhFCXBGUJRtr0qhdDsfOf/dacUpE5et9avRqdEAUc704oCvW9nQpv6qvd069atrs3S0tLgmLTJqVOnBsc0oe0kHNC5dYUJVXRQjUl9dUqFqi6n35SOGbVx9uIkXazjRgkoej36faqeJC2/sbExOM52DCHcAyQ4QyhKgjOEoiQ4QyjKXEOIRLizCkUhU0CNCzJ7nNIU33///eD4l19+6drs3bt3cEwlUXTlCN0TjYeaG7QyQSe9b9y40bVRM4FWc6hpQ4aIlvdorb9vOreDPkcyQPT3QckMej+UKKDj6hh2sz5T1BBzSp7Ss1cz0klkccuttJY3ZwhlSXCGUJQEZwhFWXh/Tgf9X5w0heo5mghWTUOlGLVaA53n6aefHhxTEgBpo5deemlw/Pnnn3dtXn755cExaQrVNKT5NNH95s2bXRvVPVQZgvY91euRxnIS5nWsHe1K5/3kk08Gx2fPnu3aqOY7duxY10a1fGv9mNB2DOoLUJUD8iUUXaxBvysdo2jOEO4BEpwhFCXBGUJREpwhFGWuIeQkHNCkrxoHJLid8ziGg64UoYl5nQjWMpSttXby5Mnus/Pnzy98j7QC5sKFC4NjKjmqY02VIZx9Nmmliq7MoEQJneQnI0e/R+aGViygyhAffvjh4Pi1117r2jhGCv0+1UQkg0y/R5Up1CCcum+Qk5Qxi7w5QyhKgjOEoiQ4QyjKwpUQHPR7pLFUBzpl6mnSWf+npyQE1aFuvzSJnL6nFekIrSRHVQb03E6VNqf8f2u9DqY2zrYBqvuoyoDeNyWsLy8vz/1Oa73mpWs5lRCmVmPU50HbfKieJb/B2fd0FnlzhlCUBGcIRUlwhlCUBGcIRVnYEJpSCYEmgnWymMT04cOHR6+t4p4EtxpJbml/NZec8v+UOKFmhrMdArUhA8hpo/foGEI0Wa7mDpV5dMyWV155ZXDsmIFk/pDRqM+Irq9G38GDB7s2Z86cGRyT0ajXdyqHLBI/eXOGUJQEZwhFSXCGUJRtJ74733MS3wnVPU6CAa18V0hL02S59oN0oGpl0mo6eU+6WK81pTJBa9wPbeckhdCzVz3rJEGQTtfFCpRc4lT6I59C21E/NBnfSdygKgfaN3quThW/WeTNGUJREpwhFCXBGUJREpwhFGXhJARnxYkKZUpCUMHtbIdAhpBO+tM9O+YKGTl6LscgI8GvfSUjQ69F96ymhLPKn6Drk3GjaPIGnccxPHSs6Tz0rBVKXlCD0FlxQmU3tQwqlcrUc0/dz3YWeXOGUJQEZwhFSXCGUJR/pRKCg2rMEydOjH5Hq9i1Ni2pm/SVk1ROGkfHiFbsU2LA2Llp7HWrAbofZyLcuR+6vmpcOs8Ufe8saHCqARLOb5gS+BUnccQZM/JoZpE3ZwhFSXCGUJQEZwhFSXCGUJRtr0ohgavCmCb4dWUCiXs1gMgUUHOHzB7HEKGV9nouEvzaN+qHTqg7Y+bsz0lbHdCEvlMaU8fE2bKBqj44yRSO+TX2ndb4WTtmk7MCSMf2m2++6droeNC19FkvYrLmzRlCURKcIRQlwRlCURKcIRTlX9mf0xG9asCcO3du9DzO3hhk9uh5yOygLBHH2HJKQSq0mkQzUJx9HKn8C/Xf2UdSzR0aa10lRHu+OKUg1Wwhs8fJviFDSL9HBpleb5H9S/4bfY60AkXbOKt/tsibM4SiJDhDKEqCM4Si+Cny24C2P9BV/Vo+srV+OwYnwcBZ5U/6ha6v+oUm3fUzaqN7eFLVB9WYNDHuaHln0n3qNhuqn5y9Lx1Ic1KChXMtTfhwKiFQm42NjYWvT16Cku0YQrgHSHCGUJQEZwhFSXCGUJRtG0IkcFXgk3Fw9uzZ0TbOXovOpLd+5poW+j2arFZzZ319vWvz8ccfj57n+PHjg2Od8G+ttS+++GJwTKaFU4aUxsjZ98MpCeOUMtGVGmT0OYkCTqKGk6jgfI9+M45Bp30lw3IWeXOGUJQEZwhFSXCGUJRta07SODoR7EzwOxPaU/cL1f/zSb9Q+X9tR3tEqu7RMv7UhpIyxr5D0JjRuXW7ATq3o90vXbo0OKbk+ClJIfQ8HF/A+T1QUojqYEp4cPS14lS4SCWEEO4BEpwhFCXBGUJREpwhFOVfWZWiK050j4/WWltdXR0cOxn9jpgmA0DFPAl3MiX0vuncTilGNRfIyNFKEGS26GfPPvts14aSF7TKA5k0zqoYp68KtSFjTXHKmVI/9NnS95zKB1PMJmcPmlRCCOEeIMEZQlESnCEUZWHNqf+L0//Zm5ubg2NnX8vbt293bXTLBtJBql9Il+o9Xrt2rWtDE/N6btKcOlm+srLStdEEdbpH1U9aPYG+RzqdKuI5ydbaD12Y0Jq3oMFJGHeqATqLHgjtK2k8/Yz2VNUKjeQBaHJNKiGE8H9CgjOEoiQ4QyhKgjOEoixsCE0pz0hiWk0BanPw4MHBMSUzqHFy6NCh0fshA4AMIWfSXceDDAg1TqiCgZYBpXvUz86fP9+1oQn+AwcODI7JxFMDhIwcNYTIIJtiilBSwN2qREDfcypjOHtvKpTcomRVSgj3AAnOEIqS4AyhKNvWnFMnkLWNaq7W+v/hL1y40LXRpG6qaKDa0dlWgT6jZArtB+kOXWlP+lrP42xl6E5o6307usfZDoJ0sZPkr7qYNKfes1PlsTWv0uAUSEurTie9P/WZtZY3ZwhlSXCGUJQEZwhFSXCGUJS5htDUfRwd1BQhk0SrA9D96Mr/qWX7CTV3nNU1hE5yk9mjn2niQGu9uULXJkNMofGYklxCJomOv5omxNQkAFpto+ei8ZjSDyqfqauvqB+OaTSLvDlDKEqCM4SiJDhDKMq2kxBIC+zYMYx50jOk35TvvvtucPzCCy+M3g9NjCv0f79T/p8S1pWlpaXRNo5up/vRvtG4OkkQ9D0dE2frPNLOmrgxdStBvT7dj5OcT9/TCgbffvtt10bHevfu3V0brd5B+lrHiBZvzCJvzhCKkuAMoSgJzhCKkuAMoShzDSHHuCBzYc+ePYNjNYha80wBneSlrQZ0z0hnMp0mr2lVikLlOx3zyzGbnH0t1Vwgs8Ppx93CKTtJBpX2g/qq40HncYwkaqNjRHuqjn2ntT7BwTE5aa/aWeTNGUJREpwhFCXBGUJRtr0FIOkp1RA7d+7s2qimcHQZaRP9v580hm5RMDUJgdro9WmS2alIp1rJ2VaCkrqdLe+mJLnTPTpj5mx5R1pN++Em+TsVAvUZqW/RmlcdUj8jPbmIxlTy5gyhKAnOEIqS4AyhKAnOEIqy7UoIZHaomNZkAmoztdy9mgJkGmnyAlUCcEwBQtvQ9XWMNEmDzkOrOVZXVwfHWhZ0Fk65SOd5OBP8ujLDqUTg4GyF4aImET1nfWY01jdu3Bgck/mjY0/G0izy5gyhKAnOEIqS4AyhKAnOEIryr2QIOXuMKGSkKJRt4pRNUQOITCwyF7QdnVuNGypnoZ/dunWra6P9p0wfHWsytsiA0X7QM1OzidAxotUcaoqcPHmya3PkyJHBMZX3oD1Wxu6nNS8bS0uu0m9v7969g2MyDJ0yl2oAHTt2bPQ7W+TNGUJREpwhFCXBGUJRtl0JwfkeaYNr164NbwQSDJyVK1QmX1FdSBUN9u/f332muo8SA1588cXBMY2Z7j1KulD1G43ZiRMnBsdHjx7t2pAO0q0dSM+ppiI9+9577w2Oab9UJ+HB8Re0Df0+yMvQ61Gp1K+++mr0HvV6zmoj0u36+1iEvDlDKEqCM4SiJDhDKEqCM4SiLLwqRSGTRk0JMgDUcCDBr+dWE6k1zxByoL46e4yokUImhRonzkoNSrhQI+fQoUNdG51gp+u9+uqrXZs33nhjcEyGkJpWtFLjueeeGxxTOdNF9qjcgsqN0LPX3wwlD+jeKHRuZ09Xvb4adq31SQjO/qlb5M0ZQlESnCEUJcEZQlEWTnx3qgPo//D0//u+ffsGx7T3pU7604SyTvw6E8q0PQTpDko+H4N0qfafEt+1OgKdZ21tbXB86tQp6/rKm2++2X2myee0ql81FSWsq8Z07sepuEGQdtXvkQbX37DznEnfq7/g6MlFkhLy5gyhKAnOEIqS4AyhKAnOEIoy1xAioTxlz06n9CAlIagpcfHixa6Nrvggc0En9JeWlro2tOpAofHQ61ESgl6fjAM1V+g8+pmbgKF9I/NLnxGtXNFnT2UeHVNE+0rXUrOH+kqJGuvr64Pj06dPd230ejQe2lcyn7Svly9f7tqokaS/13nkzRlCURKcIRQlwRlCUeZqTkpspkRiRf8/J82pCQWUIK16ThOWW2vtypUrg2OaGFdt4E56O8nPzr6a+hm1UT1JesqpLOdUByD0ek5SOV1Lx5p0ul6L2qgOpOdKz+eDDz4YHNPvdXl5eXBMfoeie7y21u87S7pUn/WZM2e6NlTRorW8OUMoS4IzhKIkOEMoSoIzhKLMVcLOin1n0t8xhFSkt9ZPFpMo1yoDNKGt5gYZGc7KBOqH9pWMHDUcnDGj+9GKEpQ44exjSeYKlf0cu0caaz03jYeOP1XKcEw0Mlc++uijwTGNh44btXF+D1evXh0cO8kln376adfm9ddfx/PnzRlCURKcIRQlwRlCUeZqTp1kJUiHqT5wJtSvX7/etXESi1VzHjx4cPRapCdIP2kSBmk8p6+qsUjzOUkITjIFnVs1LyV6O1spOv3QsaV+qMakyXt9HjT277zzTveZ/h7p+k41D+c3o2NEul3P41SG2CJvzhCKkuAMoSgJzhCKkuAMoShzDSFaBTK11KGiYprMHt0zkyZ5dWXC7t27uzYq5t19R2mlv+JsLaAGDF1fP6O+OqsnnL7RudU4cSbmyWzR75H54oyHGinvvvtu14bKXjoVJdS4oevrfVM/6NyK/j6dlU5b5M0ZQlESnCEUJcEZQlHmihinIp2r36ag/5/TpLfqF0pmUB1KFR4cLUBazUkYd7SJ6nQnUcCteqC62HmuVHVBz00J607CuN4PPVfdeuL9998fvZ/WvMoU+hypH6oxnecxpe/zyJszhKIkOEMoSoIzhKIkOEMoyn137tz5X99DCAHImzOEoiQ4QyhKgjOEoiQ4QyhKgjOEoiQ4QyjKfwBjeycid7cZIQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sad\n"
     ]
    }
   ],
   "source": [
    "# testing function\n",
    "n=9213\n",
    "display_pic_with_emotion(df['pixels'][n],df[\"emotion\"][n],emotion_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12488129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data conversion and reshaping\n",
    "X=[]\n",
    "n=len(train_data)\n",
    "#for i in range(0,len(train_data)):\n",
    "\n",
    "for i in range(0,n):    \n",
    "    img = train_data['pixels'][i]\n",
    "    val = img.split(\" \")\n",
    "    pixels = np.array(val, 'float')\n",
    "    pixels /=255\n",
    "    X.append(pixels.reshape(48,48))\n",
    "    \n",
    "X = np.array(X)\n",
    "X = X.reshape(X.shape[0],X.shape[1],X.shape[2],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e9471c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = train_data['emotion'][0:n]\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5507801d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19211,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c68ccb0",
   "metadata": {},
   "source": [
    "### Modelling and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dbf6787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend\n",
    "backend.clear_session()\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, UpSampling2D, InputLayer, Conv2DTranspose,MaxPool2D,MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a62d1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 48, 48, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 48, 48, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 12, 12, 64)        73792     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 12, 12, 32)        18464     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                73792     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 455       \n",
      "=================================================================\n",
      "Total params: 296,103\n",
      "Trainable params: 296,103\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "301/301 [==============================] - 97s 319ms/step - loss: 1.3720 - accuracy: 0.3573\n",
      "Epoch 2/10\n",
      "301/301 [==============================] - 101s 337ms/step - loss: 1.3111 - accuracy: 0.3888\n",
      "Epoch 3/10\n",
      "301/301 [==============================] - 102s 340ms/step - loss: 1.1911 - accuracy: 0.4693\n",
      "Epoch 4/10\n",
      "301/301 [==============================] - 109s 362ms/step - loss: 1.0693 - accuracy: 0.5348\n",
      "Epoch 5/10\n",
      "301/301 [==============================] - 90s 299ms/step - loss: 0.9928 - accuracy: 0.5749\n",
      "Epoch 6/10\n",
      "301/301 [==============================] - 89s 297ms/step - loss: 0.9341 - accuracy: 0.5980\n",
      "Epoch 7/10\n",
      "301/301 [==============================] - 107s 357ms/step - loss: 0.8933 - accuracy: 0.6221\n",
      "Epoch 8/10\n",
      "301/301 [==============================] - 96s 318ms/step - loss: 0.8530 - accuracy: 0.6499\n",
      "Epoch 9/10\n",
      "301/301 [==============================] - 96s 317ms/step - loss: 0.8251 - accuracy: 0.6550\n",
      "Epoch 10/10\n",
      "301/301 [==============================] - 96s 319ms/step - loss: 0.8029 - accuracy: 0.6652\n",
      "301/301 [==============================] - 29s 93ms/step - loss: 0.7362 - accuracy: 0.7002\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 48, 48, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 48, 48, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 24, 24, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 24, 24, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 12, 12, 64)        73792     \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 12, 12, 32)        18464     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                73792     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 7)                 455       \n",
      "=================================================================\n",
      "Total params: 296,103\n",
      "Trainable params: 296,103\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "301/301 [==============================] - 98s 322ms/step - loss: 1.3686 - accuracy: 0.3685\n",
      "Epoch 2/10\n",
      "301/301 [==============================] - 81s 268ms/step - loss: 1.3275 - accuracy: 0.3813\n",
      "Epoch 3/10\n",
      "301/301 [==============================] - 83s 277ms/step - loss: 1.2070 - accuracy: 0.4602\n",
      "Epoch 4/10\n",
      "301/301 [==============================] - 85s 283ms/step - loss: 1.0660 - accuracy: 0.5401\n",
      "Epoch 5/10\n",
      "301/301 [==============================] - 86s 285ms/step - loss: 0.9638 - accuracy: 0.5857\n",
      "Epoch 6/10\n",
      "301/301 [==============================] - 85s 283ms/step - loss: 0.9114 - accuracy: 0.6111\n",
      "Epoch 7/10\n",
      "301/301 [==============================] - 87s 290ms/step - loss: 0.8588 - accuracy: 0.6426\n",
      "Epoch 8/10\n",
      "301/301 [==============================] - 86s 284ms/step - loss: 0.8316 - accuracy: 0.6543\n",
      "Epoch 9/10\n",
      "301/301 [==============================] - 86s 285ms/step - loss: 0.7881 - accuracy: 0.6754\n",
      "Epoch 10/10\n",
      "301/301 [==============================] - 87s 288ms/step - loss: 0.7721 - accuracy: 0.6843\n",
      "301/301 [==============================] - 26s 84ms/step - loss: 0.7566 - accuracy: 0.6838\n"
     ]
    }
   ],
   "source": [
    "#building convoluted network\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=2, shuffle=True)\n",
    "\n",
    "#k-fold initialize\n",
    "fold_no = 1\n",
    "\n",
    "#Initialize CNN\n",
    "for train, test in kfold.split(X, Y):\n",
    "    cnn = Sequential([\n",
    "        layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(48, 48, 1),padding='same'),\n",
    "        layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu',padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.5),\n",
    "    \n",
    "        layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu',padding='same'),\n",
    "        layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu',padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.5),\n",
    "        \n",
    "        layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu',padding='same'),\n",
    "        layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu',padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.5),\n",
    "    \n",
    "    \n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(7, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    cnn.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    cnn.summary()\n",
    "    \n",
    "    #fit model\n",
    "    cnn.fit(X[train], Y[train], epochs=10)\n",
    "    \n",
    "    #evaluate model\n",
    "    cnn.evaluate(X[test],Y[test])\n",
    "    \n",
    "    fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677e6a48",
   "metadata": {},
   "source": [
    "### Testing - public dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b709d85a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>28709</td>\n",
       "      <td>0</td>\n",
       "      <td>254 254 254 254 254 249 255 160 2 58 53 70 77 ...</td>\n",
       "      <td>PublicTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>28711</td>\n",
       "      <td>4</td>\n",
       "      <td>69 118 61 60 96 121 103 87 103 88 70 90 115 12...</td>\n",
       "      <td>PublicTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>28713</td>\n",
       "      <td>3</td>\n",
       "      <td>87 79 74 66 74 96 77 80 80 84 83 89 102 91 84 ...</td>\n",
       "      <td>PublicTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>28714</td>\n",
       "      <td>3</td>\n",
       "      <td>235 233 223 109 34 37 34 31 28 38 56 69 106 13...</td>\n",
       "      <td>PublicTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>28716</td>\n",
       "      <td>0</td>\n",
       "      <td>176 177 170 168 173 171 167 169 166 139 98 107...</td>\n",
       "      <td>PublicTest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   level_0  index  emotion                                             pixels  \\\n",
       "0        0  28709        0  254 254 254 254 254 249 255 160 2 58 53 70 77 ...   \n",
       "1        2  28711        4  69 118 61 60 96 121 103 87 103 88 70 90 115 12...   \n",
       "2        4  28713        3  87 79 74 66 74 96 77 80 80 84 83 89 102 91 84 ...   \n",
       "3        5  28714        3  235 233 223 109 34 37 34 31 28 38 56 69 106 13...   \n",
       "4        7  28716        0  176 177 170 168 173 171 167 169 166 139 98 107...   \n",
       "\n",
       "        Usage  \n",
       "0  PublicTest  \n",
       "1  PublicTest  \n",
       "2  PublicTest  \n",
       "3  PublicTest  \n",
       "4  PublicTest  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "public_test = public_test[(public_test[\"emotion\"]==0) | (public_test[\"emotion\"]==3) | (public_test[\"emotion\"]==4) | (public_test[\"emotion\"]==5) ]\n",
    "public_test = public_test.reset_index()\n",
    "public_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "092850ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=len(public_test)\n",
    "X_test=[]\n",
    "Y_test=[]\n",
    "\n",
    "for i in range(0,n):    \n",
    "    img = public_test['pixels'][i]\n",
    "    val = img.split(\" \")\n",
    "    pixels = np.array(val, 'float')\n",
    "    pixels /=255\n",
    "    X_test.append(pixels.reshape(48,48))\n",
    "    \n",
    "X_test = np.array(X_test)\n",
    "X_test = X_test.reshape(X_test.shape[0],X_test.shape[1],X_test.shape[2],1)\n",
    "\n",
    "Y_test = public_test['emotion'][0:n]\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e062f002",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cnn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cbe84ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.26332980e-01, 4.64098378e-07, 1.10833454e-07, 1.35399476e-01,\n",
       "        4.14913118e-01, 2.33536903e-02, 1.82842939e-07],\n",
       "       [3.81589770e-01, 2.42829674e-06, 4.72980105e-07, 1.27941417e-02,\n",
       "        5.70664525e-01, 3.49469371e-02, 1.72372586e-06],\n",
       "       [7.93764368e-03, 3.52311652e-11, 8.51169690e-11, 9.88796353e-01,\n",
       "        1.72299123e-03, 1.54305727e-03, 7.79632303e-10],\n",
       "       [4.28495789e-03, 1.38789397e-10, 3.84790338e-10, 9.79614437e-01,\n",
       "        4.31255065e-03, 1.17880860e-02, 1.63686853e-09],\n",
       "       [1.51198491e-01, 1.74091037e-05, 6.14425835e-06, 1.04877032e-01,\n",
       "        6.97360456e-01, 4.65310402e-02, 9.38271660e-06]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2cef7f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 4, 3, 3, 4, 5, 3, 0, 4, 5]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_classes = [np.argmax(element) for element in y_pred]\n",
    "y_classes[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e754bb90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4, 3, 3, 0, 0, 3, 0, 4, 5], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "561bf04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.34      0.45       467\n",
      "           3       0.77      0.86      0.82       895\n",
      "           4       0.59      0.74      0.65       653\n",
      "           5       0.81      0.74      0.77       415\n",
      "\n",
      "    accuracy                           0.71      2430\n",
      "   macro avg       0.71      0.67      0.67      2430\n",
      "weighted avg       0.71      0.71      0.69      2430\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix , classification_report\n",
    "print(\"Classification Report: \\n\", classification_report(Y_test, y_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8880c79",
   "metadata": {},
   "source": [
    "### Testing - private dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c2c00b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32298</td>\n",
       "      <td>0</td>\n",
       "      <td>170 118 101 88 88 75 78 82 66 74 68 59 63 64 6...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32299</td>\n",
       "      <td>5</td>\n",
       "      <td>7 5 8 6 7 3 2 6 5 4 4 5 7 5 5 5 6 7 7 7 10 10 ...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32301</td>\n",
       "      <td>4</td>\n",
       "      <td>200 197 149 139 156 89 111 58 62 95 113 117 11...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32303</td>\n",
       "      <td>0</td>\n",
       "      <td>138 142 66 80 87 92 97 99 88 73 72 83 92 102 1...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32304</td>\n",
       "      <td>4</td>\n",
       "      <td>72 66 66 69 62 51 57 60 56 66 63 70 68 68 81 9...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  emotion                                             pixels  \\\n",
       "0  32298        0  170 118 101 88 88 75 78 82 66 74 68 59 63 64 6...   \n",
       "1  32299        5  7 5 8 6 7 3 2 6 5 4 4 5 7 5 5 5 6 7 7 7 10 10 ...   \n",
       "2  32301        4  200 197 149 139 156 89 111 58 62 95 113 117 11...   \n",
       "3  32303        0  138 142 66 80 87 92 97 99 88 73 72 83 92 102 1...   \n",
       "4  32304        4  72 66 66 69 62 51 57 60 56 66 63 70 68 68 81 9...   \n",
       "\n",
       "         Usage  \n",
       "0  PrivateTest  \n",
       "1  PrivateTest  \n",
       "2  PrivateTest  \n",
       "3  PrivateTest  \n",
       "4  PrivateTest  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "private_test = private_test[(private_test[\"emotion\"]==0) | (private_test[\"emotion\"]==3) | (private_test[\"emotion\"]==4) | (private_test[\"emotion\"]==5) ]\n",
    "private_test = private_test.reset_index()\n",
    "private_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e069597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=len(private_test)\n",
    "X_ptest=[]\n",
    "Y_ptest=[]\n",
    "\n",
    "for i in range(0,n):    \n",
    "    img = private_test['pixels'][i]\n",
    "    val = img.split(\" \")\n",
    "    pixels = np.array(val, 'float')\n",
    "    pixels /=255\n",
    "    X_ptest.append(pixels.reshape(48,48))\n",
    "    \n",
    "X_ptest = np.array(X_ptest)\n",
    "X_ptest = X_ptest.reshape(X_ptest.shape[0],X_ptest.shape[1],X_ptest.shape[2],1)\n",
    "\n",
    "Y_ptest = private_test['emotion'][0:n]\n",
    "Y_ptest = np.array(Y_ptest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3696dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.30      0.42       491\n",
      "           3       0.73      0.87      0.80       879\n",
      "           4       0.53      0.69      0.60       594\n",
      "           5       0.84      0.69      0.76       416\n",
      "\n",
      "    accuracy                           0.68      2380\n",
      "   macro avg       0.69      0.64      0.64      2380\n",
      "weighted avg       0.69      0.68      0.66      2380\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = cnn.predict(X_ptest)\n",
    "y_classes = [np.argmax(element) for element in y_pred]\n",
    "print(\"Classification Report: \\n\", classification_report(Y_ptest, y_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824ac1d1",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd06d93",
   "metadata": {},
   "source": [
    "The overall accuracy of the model is around 70%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef3bed1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
